{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28c55724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "sys.path.append('../../scripts')\n",
    "import featurizer\n",
    "sys.path.append('../../../../../scripts/preprocessing')\n",
    "import preprocessing\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "from torch import nn\n",
    "from typing import Union,Optional\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c26768ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN : \n",
      "\n",
      "timestamp    0\n",
      "open         0\n",
      "high         0\n",
      "low          0\n",
      "close        0\n",
      "volume       0\n",
      "returns      1\n",
      "dtype: int64\n",
      "\n",
      "Total NaN : \n",
      "\n",
      "timestamp    0\n",
      "open         0\n",
      "high         0\n",
      "low          0\n",
      "close        0\n",
      "volume       0\n",
      "returns      1\n",
      "dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d1096b03cc470eab63112497e720eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We make use of 90.44726670347875% of the data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616c2c362d8246999c68d1c186dd5b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_btc = preprocessing.load_df('../../../../../../data/Binance/BTC_USDT.csv')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path_btc = '../../../../../../data/data_new/raw/Binance/BTC_USDT/BTC_USDT.csv'\n",
    "\n",
    "df_btc_feat = featurizer.load_and_featurize(path_btc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0aed776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NaN : \n",
      "\n",
      "timestamp    0\n",
      "open         0\n",
      "high         0\n",
      "low          0\n",
      "close        0\n",
      "volume       0\n",
      "returns      1\n",
      "dtype: int64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7443f8818e9c40e6b36d760a00d28b2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1811 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We make use of 90.44726670347875% of the data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b6c2ecd62d42e1bd936bc332fbee65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_eth = '../../../../../../data/data_new/raw/Binance/ETH_USDT/ETH_USDT.csv'\n",
    "\n",
    "df_eth_feat = featurizer.load_and_featurize(path_eth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11bb1eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class twoday_basic_dset(Dataset):\n",
    "    \n",
    "    def __init__(self,df):\n",
    "        self.X = torch.tensor(df.X.tolist(),dtype = torch.float)\n",
    "        self.y = torch.tensor(df.y.astype(int).tolist(),dtype = torch.float)\n",
    "        \n",
    "    def __getitem__(self,\n",
    "                    index: int):\n",
    "        return (self.X[index],self.y[index])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e174927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dims: Union[int, list],\n",
    "        num_layers: Optional[int] = None,\n",
    "        activation: str = \"relu\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        if isinstance(embed_dims, int):\n",
    "            assert num_layers is not None\n",
    "            embed_dims = [embed_dims] * num_layers\n",
    "        else:\n",
    "            assert num_layers is None\n",
    "            num_layers = len(embed_dims)\n",
    "\n",
    "        assert activation == \"relu\"\n",
    "\n",
    "        self.embed_dims = embed_dims\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                sublayer\n",
    "                for i, (m, n) in enumerate(zip(embed_dims, embed_dims[1:]))\n",
    "                for sublayer in [nn.Linear(m, n), nn.BatchNorm1d(n), nn.ReLU()]\n",
    "                if not (i == num_layers - 2 and isinstance(sublayer, nn.ReLU))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c63249e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b66ed49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pytorch_lightning as pl\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import TQDMProgressBar\n",
    "#from pytorch_lightning import LightningModule, Trainer\n",
    "#from pytorch_lightning.callbacks.progress import TQDMProgressBar\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "#from pytorch_lightning.loggers import CSVLogger\n",
    "#import torchmetrics.functional.classification as tm\n",
    "import torchmetrics as tm\n",
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP_binary_classifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_dims: List[int],\n",
    "                 lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.mlp = MLP(embed_dims)\n",
    "        self.Classifier_head = nn.Linear(embed_dims[-1],1)\n",
    "        self.s = nn.Sigmoid()\n",
    "        self.step_outputs = {\"train\": [],\"val\":[],\"test\":[]}\n",
    "        self.average_precision = tm.AveragePrecision(task = 'binary')\n",
    "        self.auroc = {name : tm.AUROC(task = 'binary') for name in ['train','val']}\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self,\n",
    "                batch):\n",
    "        \n",
    "        x,y = batch\n",
    "        x = self.mlp(x)\n",
    "        x = self.Classifier_head(x)\n",
    "        x = self.s(x)\n",
    "        \n",
    "        return x\n",
    "          \n",
    "    def _step(self,\n",
    "              name: str,\n",
    "              batch: List[torch.tensor]):\n",
    "        \n",
    "        target = batch[1].to(device)\n",
    "        predicted_proba = self.forward(batch).flatten()\n",
    "        criterion = nn.BCELoss()\n",
    "        loss = criterion(predicted_proba,target)\n",
    "        self.step_outputs[name].append({\"loss\": loss, \"predicted_proba\": predicted_proba, \"target\": batch[1]})\n",
    "        \n",
    "        self.log(\n",
    "            f\"{name}/loss\", loss, prog_bar = True, on_epoch=True, logger = True, on_step= (name==\"train\"), batch_size=batch[1].shape[0]\n",
    "        )\n",
    "        \n",
    "        #if name == 'val':\n",
    "        #    self.log(\"val/average_precision\",self.average_precision,on_epoch = True,on_step = False)\n",
    "        \n",
    "        \n",
    "        return {\"loss\": loss, \"predicted_proba\": predicted_proba, \"target\": batch[1]}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx = None):\n",
    "        return self._step(\"train\", batch)\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx = None):\n",
    "        return self._step(\"val\", batch)\n",
    "    \n",
    "    #def _epoch_end(self, name):\n",
    "    #    y = torch.hstack([output[\"target\"] for output in self.step_outputs[name]])\n",
    "    #    y = y.type(torch.long)\n",
    "    #    print(y)\n",
    "    #    preds = torch.hstack([output[\"predicted_proba\"] for output in self.step_outputs[name]])\n",
    "    #    print(preds) \n",
    "    \n",
    "        #self.log(f\"{name}/auroc\", self.auroc[name])\n",
    "        #self.log(f\"{name}/average_precision\", tm.binary_average_precision(preds, y))\n",
    "\n",
    "        #precision, recall, _ = tm.binary_precision_recall_curve(preds, y)\n",
    "        #for ten_x in range(1,11):\n",
    "        #    x = ten_x / 10\n",
    "        #    if compute_p_at_r(precision, recall, x) != None:\n",
    "        #        self.log(f\"{name}/p@r={x}\", compute_p_at_r(precision, recall, x))\n",
    "#\n",
    "        #    y_pred = (preds > x).long()\n",
    "        #    self.log(f\"{name}/accuracy@thh={x}\", tm.binary_accuracy(y_pred, y))\n",
    "        #    self.log(f\"{name}/recall@thh={x}\", tm.binary_recall(y_pred, y))\n",
    "        #    self.log(f\"{name}/precision@thh={x}\", tm.binary_precision(y_pred, y))\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "        return optimizer  \n",
    "    \n",
    "    #def on_training_epoch_end(self):\n",
    "    #    outputs = self.step_outputs['train']\n",
    "    #    return self._epoch_end(\"train\")\n",
    "    #\n",
    "    #def on_validation_epoch_end(self):\n",
    "    #    outputs = self.step_outputs['val']\n",
    "    #    return self._epoch_end(\"val\")\n",
    "    \n",
    "    #def train_dataloader(self):\n",
    "    #    dloader = dd.get_dataloader(self.batch_size,self.trainset,self.esm_feat,is_inference = False)\n",
    "    #    return dloader\n",
    "    #\n",
    "    #def val_dataloader(self):\n",
    "    #    dloader = dd.get_dataloader(self.batch_size,self.valset,self.esm_feat,is_inference = True)\n",
    "    #    return dloader\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70b6812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MyCallback(pl.Callback):\n",
    "#    \n",
    "#    def on_x_epoch_end(self, name, pl_module):\n",
    "#        # do something with all training_step outputs, for example:\n",
    "#        epoch_mean = torch.stack(pl_module.step_outputs[name]).mean()\n",
    "#        pl_module.log(f\"{name}_epoch_mean\", epoch_mean)\n",
    "#        # free up the memory\n",
    "#        pl_module.step_outputs[name].clear()\n",
    "#        \n",
    "#    def on_train_epoch_end(self, trainer, pl_module):\n",
    "#        return self.on_x_epoch_end(\"train\", pl_module)\n",
    "#        \n",
    "#    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "#        return self.on_x_epoch_end(\"val\", pl_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "301e8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dloader(df,\n",
    "                batch_size,\n",
    "                num_workers = 64,\n",
    "                shuffle = True):\n",
    "    \n",
    "    dset = twoday_basic_dset(df)\n",
    "    dloader = DataLoader(dset,batch_size = batch_size,num_workers = num_workers, shuffle = shuffle)\n",
    "    \n",
    "    return dloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ab9faefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_btc_feat[:int(0.6*len(df_btc_feat))]\n",
    "val_df = df_btc_feat[int(0.62*len(df_btc_feat)):int(0.75*len(df_btc_feat))]\n",
    "test_df = df_btc_feat[int(0.75*len(df_btc_feat)):]\n",
    "\n",
    "train_loader = get_dloader(train_df,16,64,True)\n",
    "val_loader =  get_dloader(val_df,256,64,True)\n",
    "test_df = get_dloader(test_df,256,64,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07e1bb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_binary_classifier([124,64,16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0ae3479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop_callback = EarlyStopping(monitor=\"val/average_precision\", min_delta=0.01, patience=5, verbose=False, mode=\"max\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842cf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type                   | Params\n",
      "-------------------------------------------------------------\n",
      "0 | mlp               | MLP                    | 9.2 K \n",
      "1 | Classifier_head   | Linear                 | 17    \n",
      "2 | s                 | Sigmoid                | 0     \n",
      "3 | average_precision | BinaryAveragePrecision | 0     \n",
      "-------------------------------------------------------------\n",
      "9.2 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.2 K     Total params\n",
      "0.037     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737c9d1b91f44b968e923ba2c0345599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator=\"gpu\") #,callbacks = [early_stop_callback])\n",
    "trainer.fit(model,train_loader,val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090483a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train/loss_step</th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/auroc</th>\n",
       "      <th>val/average_precision</th>\n",
       "      <th>train/loss_epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.626697</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0.509367</td>\n",
       "      <td>0.522436</td>\n",
       "      <td>0.822361</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.667748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.607575</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "      <td>0.512355</td>\n",
       "      <td>0.529964</td>\n",
       "      <td>0.825397</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>NaN</td>\n",
       "      <td>62</td>\n",
       "      <td>3905</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>0.418630</td>\n",
       "      <td>63</td>\n",
       "      <td>3949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>3967</td>\n",
       "      <td>1.809072</td>\n",
       "      <td>0.514660</td>\n",
       "      <td>0.828295</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>NaN</td>\n",
       "      <td>63</td>\n",
       "      <td>3967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.420627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>0.317282</td>\n",
       "      <td>64</td>\n",
       "      <td>3999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train/loss_step  epoch  step  val/loss  val/auroc  val/average_precision  \\\n",
       "0           0.626697      0    49       NaN        NaN                    NaN   \n",
       "1                NaN      0    61  0.509367   0.522436               0.822361   \n",
       "2                NaN      0    61       NaN        NaN                    NaN   \n",
       "3           0.607575      1    99       NaN        NaN                    NaN   \n",
       "4                NaN      1   123  0.512355   0.529964               0.825397   \n",
       "..               ...    ...   ...       ...        ...                    ...   \n",
       "203              NaN     62  3905       NaN        NaN                    NaN   \n",
       "204         0.418630     63  3949       NaN        NaN                    NaN   \n",
       "205              NaN     63  3967  1.809072   0.514660               0.828295   \n",
       "206              NaN     63  3967       NaN        NaN                    NaN   \n",
       "207         0.317282     64  3999       NaN        NaN                    NaN   \n",
       "\n",
       "     train/loss_epoch  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2            0.667748  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "..                ...  \n",
       "203          0.420888  \n",
       "204               NaN  \n",
       "205               NaN  \n",
       "206          0.420627  \n",
       "207               NaN  \n",
       "\n",
       "[208 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./lightning_logs/version_27/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cadd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.read_csv('./lightning_logs/version_16/metrics.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
